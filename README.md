can - A simple dense matrix-matrix mutiplication benchmark.
======
there are serial, intel MKL dgemm(), OpenMP, MPI and hybrid versions.  
MPI version is based on Cannon's algorithm.  
intel compiler and intel MKL library are needed.  
Input matrix is a psudorandom number, that is generated by intel MKL Mersenne Twister(MT19937)  
  
- binary names:  
  - serial: seri  
  - OpenMP: omp  
  - intel MKL dgemm(): dgemm  
  - MPI: can  
  - hybrid: can_hyb  
  
- matrix size: imax x imax (param.f)  
  
- Some notes for MPI and hybrid version:
  - imax/sqrt(np) must be an integer.
  - sqrt(np) must be an integer.

how to run
-------
* intel compiler and intel MPI are required.
~~~
$ make
$ ./create_input

$ ./seri
or
$ ./omp
or
./dgemm
or
mpirun -np $NP ./can
or
mpirun -np $NP ./can_hyb
~~~

performance comparison(matrix size: 4096x4096, Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, 14 cores/socket, 2 sockets/node, 4 nodes, intel OPA):
-------

* serial
~~~
$ ./seri
 serial time:   6.99500107765198        19.6481675908668      Gflops
 trace:   4196462.48061815
~~~
* MKL dgemm() (single thread)
~~~
$ MKL_NUM_THREADS=1 ./dgemm
 dgemm time:   3.69211506843567        37.2249918879782      Gflops
 trace:   4196462.48061815
~~~
* MKL dgemm() (28 threads)
~~~
$ MKL_NUM_THREADS=28 KMP_AFFINITY=compact ./dgemm
 dgemm time:   1.08629608154297        126.520711808868      Gflops
 trace:   4196462.48061815
~~~
* OpenMP (28 threads)
~~~
$ OMP_NUM_THREADS=28 KMP_AFFINITY=compact ./omp
 omp time:  0.852473020553589        161.223816071913      Gflops
 trace:   4196462.48061815
~~~
* MPI
~~~
$ mpiexec.hydra -ppn 16 -np 64 ./can
 MPI time:  0.405706882476807        338.764165480622      Gflops
 trace:   4196462.48061815
~~~
* hybrid
~~~
$ OMP_NUM_THREADS=$((28/4)) KMP_AFFINITY=compact mpiexec.hydra -ppn 4 -np 16 ./can_hyb
 MPI time:  0.325567960739136        422.151347939683      Gflops
 trace:   4196462.48061815
~~~
check the results
-------
~~~
$ ./check c.seri c.dgemm
 maximum error:  9.094947017729282E-012
$ ./check c.seri c.omp
 maximum error:  0.000000000000000E+000
$ ./check c.seri c.can
 maximum error:  1.409716787748039E-011
$ ./check c.seri c.can_hyb
 maximum error:  1.409716787748039E-011
~~~

OpenACC version
-------
PGI compiler, OpenMPI and intel MKL are required.
CPU and inteterconnect are the same as normal version, GPU is nvidia P100x4 per 1 node.  
~~~
$ make -f makefile.acc.mk
$ ./create_input
$ ./seri
./seri
 serial time:    51.39800500000000         2.674013387718064      Gflops
 trace:    4196462.480618147
$  mpirun -x LD_LIBRARY_PATH -npernode 4 -np 16 ./can_acc
 imax:         4096
 MPI time:   0.2325633987784386         590.9741351988800      Gflops
 trace:    4196462.480618145
$ ./check c.seri c.can_acc
 maximum error:   1.4097167877480388E-011
~~~

Large size test(16384x16384, 4nodes)
-------
* hybrid(MPI+OpenMP), intel compiler and intel MPI
~~~
OMP_NUM_THREADS=$((28/4)) KMP_AFFINITY=compact mpiexec.hydra -ppn 4 -np 16 ./can_hyb
 MPI time:   43.2524101734161        203.366540429561      Gflops
 trace:   67116321.7059676
~~~
* hybrid(MPI+OpenACC), PGI compiler and OpenMPI
-------
~~~
$ mpirun -x LD_LIBRARY_PATH -npernode 4 -np 16 ./can_acc
 MPI time:    19.99106211587787         440.0012851354065      Gflops
 trace:    67116321.70596762
~~~
